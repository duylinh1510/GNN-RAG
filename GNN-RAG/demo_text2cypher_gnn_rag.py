"""
Demo Text2Cypher GNN-RAG Ä‘Æ¡n giáº£n
=====================================

Demo nÃ y minh há»a luá»“ng xá»­ lÃ½ cá»§a Text2Cypher GNN-RAG:
1. XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng trong cÃ¢u há»i
2. TÃ¬m node Ä‘á»‘i tÆ°á»£ng trong graph
3. Sá»­ dá»¥ng GNN Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng Ä‘i giá»¯a cÃ¡c node
4. Xuáº¥t Ä‘Æ°á»ng Ä‘i lÃ m context cho viá»‡c sinh Cypher query

Dá»¯ liá»‡u máº«u: Há»‡ thá»‘ng quáº£n lÃ½ Ä‘Æ¡n hÃ ng (Products, Categories, Suppliers, Customers, Orders)
"""

import re
import json
from typing import List, Dict, Tuple, Set
import networkx as nx
import numpy as np
from dataclasses import dataclass

# Advanced imports (sáº½ dÃ¹ng khi cÃ³ dependencies)
try:
    import torch
    import torch.nn.functional as F
    from torch_geometric.nn import GCNConv
    from torch_geometric.data import Data
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    ADVANCED_AVAILABLE = True
    print("âœ… Advanced dependencies available (PyTorch, PyG, SentenceTransformers)")
except ImportError:
    ADVANCED_AVAILABLE = False
    print("âš ï¸ Advanced dependencies not found. Install: torch, torch-geometric, sentence-transformers, scikit-learn")

@dataclass
class Entity:
    """Lá»›p Ä‘áº¡i diá»‡n cho má»™t thá»±c thá»ƒ Ä‘Æ°á»£c nháº­n diá»‡n"""
    name: str
    entity_type: str
    confidence: float

class EntityExtractor:
    """BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng trong cÃ¢u há»i"""
    
    def __init__(self):
        # Tá»« Ä‘iá»ƒn cÃ¡c tá»« khÃ³a cho tá»«ng loáº¡i thá»±c thá»ƒ
        self.entity_keywords = {
            'Product': [
                'sáº£n pháº©m', 'hÃ ng hÃ³a', 'máº·t hÃ ng', 'chai', 'chang', 'beer', 'bia',
                'product', 'products', 'item', 'goods', 'Ä‘á»“ uá»‘ng', 'thá»©c Äƒn', 'food', 'drink'
            ],
            'Category': [
                'danh má»¥c', 'loáº¡i', 'category', 'nhÃ³m', 'phÃ¢n loáº¡i',
                'beverages', 'Ä‘á»“ uá»‘ng', 'condiments', 'gia vá»‹', 'seafood', 'háº£i sáº£n'
            ],
            'Customer': [
                'khÃ¡ch hÃ ng', 'customer', 'customers','client', 'ngÆ°á»i mua', 'cÃ´ng ty'
            ],
            'Supplier': [
                'nhÃ  cung cáº¥p', 'supplier', 'vendor', 'nhÃ  phÃ¢n phá»‘i'
            ],
            'Order': [
                'Ä‘Æ¡n hÃ ng', 'order', 'orders', 'hÃ³a Ä‘Æ¡n', 'mua', 'bÃ¡n', 'Ä‘áº·t hÃ ng'
            ]
        }
    
    def extract_entities(self, question: str) -> List[Entity]:
        """TrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« cÃ¢u há»i"""
        question_lower = question.lower()
        entities = []
        
        for entity_type, keywords in self.entity_keywords.items():
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    confidence = 0.8 if len(keyword) > 3 else 0.6
                    entities.append(Entity(keyword, entity_type, confidence))
        
        # Loáº¡i bá» trÃ¹ng láº·p vÃ  sáº¯p xáº¿p theo Ä‘á»™ tin cáº­y
        unique_entities = {}
        for entity in entities:
            key = (entity.name, entity.entity_type)
            if key not in unique_entities or entity.confidence > unique_entities[key].confidence:
                unique_entities[key] = entity
        
        return sorted(unique_entities.values(), key=lambda x: x.confidence, reverse=True)

class GraphBuilder:
    """BÆ°á»›c 2: XÃ¢y dá»±ng vÃ  quáº£n lÃ½ knowledge graph"""
    
    def __init__(self):
        self.graph = nx.Graph()
        self.node_attributes = {}
        self.build_sample_graph()
    
    def build_sample_graph(self):
        """XÃ¢y dá»±ng graph máº«u tá»« schema"""
        # ThÃªm nodes máº«u
        sample_data = {
            'Product': [
                {'id': 'P1', 'name': 'Chai', 'price': 18.0, 'categoryID': 'C1'},
                {'id': 'P2', 'name': 'Chang', 'price': 19.0, 'categoryID': 'C1'},
                {'id': 'P3', 'name': 'Aniseed Syrup', 'price': 10.0, 'categoryID': 'C2'}
            ],
            'Category': [
                {'id': 'C1', 'name': 'Beverages', 'description': 'Soft drinks, coffees, teas, beers, and ales'},
                {'id': 'C2', 'name': 'Condiments', 'description': 'Sweet and savory sauces'}
            ],
            'Customer': [
                {'id': 'CUST1', 'companyName': 'Alfreds Futterkiste', 'contactName': 'Maria Anders', 'city': 'Berlin', 'country': 'Germany', 'phone': '030-0074321'},
                {'id': 'CUST2', 'companyName': 'Ana Trujillo', 'contactName': 'Ana Trujillo', 'city': 'Mexico D.F.', 'country': 'Mexico', 'phone': '(5) 555-4729'}
            ],
            'Supplier': [
                {'id': 'S1', 'companyName': 'Exotic Liquids', 'contactName': 'Charlotte Cooper', 'city': 'London', 'country': 'UK', 'phone': '(171) 555-2222'},
                {'id': 'S2', 'companyName': 'New Orleans Cajun', 'contactName': 'Shelley Burke', 'city': 'New Orleans', 'country': 'USA', 'phone': '(100) 555-4822'}
            ],
            'Order': [
                {'id': 'O1', 'orderDate': '2024-01-15', 'customerID': 'CUST1'},
                {'id': 'O2', 'orderDate': '2024-01-16', 'customerID': 'CUST2'}
            ]
        }
        
        # ThÃªm nodes vÃ o graph
        for node_type, nodes in sample_data.items():
            for node in nodes:
                node_id = f"{node_type}_{node['id']}"
                self.graph.add_node(node_id, type=node_type, **node)
                self.node_attributes[node_id] = {**node, 'type': node_type}
        
        # ThÃªm relationships
        relationships = [
            ('Product_P1', 'Category_C1', 'PART_OF'),
            ('Product_P2', 'Category_C1', 'PART_OF'),
            ('Product_P3', 'Category_C2', 'PART_OF'),
            ('Supplier_S1', 'Product_P1', 'SUPPLIES'),
            ('Supplier_S2', 'Product_P2', 'SUPPLIES'),
            ('Customer_CUST1', 'Order_O1', 'PURCHASED'),
            ('Customer_CUST2', 'Order_O2', 'PURCHASED'),
            ('Order_O1', 'Product_P1', 'ORDERS'),
            ('Order_O2', 'Product_P2', 'ORDERS')
        ]
        
        for source, target, rel_type in relationships:
            self.graph.add_edge(source, target, relation=rel_type)
    
    def find_entity_nodes(self, entities: List[Entity]) -> List[str]:
        """TÃ¬m cÃ¡c node trong graph tÆ°Æ¡ng á»©ng vá»›i entities"""
        matched_nodes = []
        
        for entity in entities:
            for node_id, attrs in self.node_attributes.items():
                if attrs['type'] == entity.entity_type:
                    # Náº¿u lÃ  tá»« khÃ³a chung (nhÆ° "product", "customer"), láº¥y táº¥t cáº£ nodes cÃ¹ng loáº¡i
                    if entity.name.lower() in ['product', 'sáº£n pháº©m', 'customer', 'khÃ¡ch hÃ ng', 
                                              'category', 'danh má»¥c', 'supplier', 'nhÃ  cung cáº¥p', 
                                              'order', 'Ä‘Æ¡n hÃ ng']:
                        matched_nodes.append(node_id)
                    # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho "Ä‘á»“ uá»‘ng" - tÃ¬m Category Beverages vÃ  Product liÃªn quan
                    elif entity.name.lower() in ['Ä‘á»“ uá»‘ng', 'drink', 'beverage', 'beverages']:
                        if attrs['type'] == 'Category' and 'beverages' in str(attrs.get('name', '')).lower():
                            matched_nodes.append(node_id)
                        elif attrs['type'] == 'Product' and attrs.get('categoryID') == 'C1':  # C1 = Beverages
                            matched_nodes.append(node_id)
                    # Náº¿u lÃ  tÃªn cá»¥ thá»ƒ, tÃ¬m chÃ­nh xÃ¡c
                    elif (entity.name.lower() in str(attrs.get('name', '')).lower() or
                          entity.name.lower() in str(attrs.get('companyName', '')).lower() or
                          entity.name.lower() in str(attrs.get('description', '')).lower()):
                        matched_nodes.append(node_id)
        
        # Loáº¡i bá» trÃ¹ng láº·p
        return list(set(matched_nodes))

class AdvancedGCN(torch.nn.Module):
    """Graph Convolutional Network Ä‘á»ƒ há»c node embeddings"""
    
    def __init__(self, input_dim, hidden_dim=128, output_dim=64, num_layers=2):
        super(AdvancedGCN, self).__init__()
        self.num_layers = num_layers
        
        # CÃ¡c layers GCN
        self.convs = torch.nn.ModuleList()
        self.convs.append(GCNConv(input_dim, hidden_dim))
        
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
        
        self.convs.append(GCNConv(hidden_dim, output_dim))
        self.dropout = torch.nn.Dropout(0.2)
    
    def forward(self, x, edge_index):
        """Forward pass qua GCN"""
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i < len(self.convs) - 1:
                x = F.relu(x)
                x = self.dropout(x)
        return x

class GNNPathFinder:
    """BÆ°á»›c 3: Sá»­ dá»¥ng GNN vá»›i sentence-transformers vÃ  PyTorch Geometric"""
    
    def __init__(self, graph: nx.Graph, graph_builder):
        self.graph = graph
        self.graph_builder = graph_builder
        
        if not ADVANCED_AVAILABLE:
            print("âš ï¸ Fallback to simple embeddings - install advanced dependencies for full features")
            self.node_embeddings = self._initialize_simple_embeddings()
            self.enhanced_embeddings = self.node_embeddings
        else:
            # Táº¡o sentence embeddings
            self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
            print(f"âœ… Loaded sentence transformer vá»›i embedding dim: {self.sentence_model.get_sentence_embedding_dimension()}")
            
            # Táº¡o text embeddings cho nodes
            self.text_embeddings = self._create_text_embeddings()
            
            # Chuáº©n bá»‹ dá»¯ liá»‡u PyTorch Geometric
            self.pyg_data = self._prepare_pyg_data()
            
            # Train GCN vÃ  táº¡o enhanced embeddings
            self.gcn_model = self._train_gcn()
            self.enhanced_embeddings = self._create_enhanced_embeddings()
    
    def _initialize_simple_embeddings(self) -> Dict[str, np.ndarray]:
        """Fallback: Khá»Ÿi táº¡o embeddings Ä‘Æ¡n giáº£n cho cÃ¡c nodes"""
        embeddings = {}
        embedding_dim = 64
        
        for node in self.graph.nodes():
            embeddings[node] = np.random.normal(0, 1, embedding_dim)
        
        return embeddings
    
    def _encode_node_text(self, node_attrs: Dict) -> str:
        """Chuyá»ƒn Ä‘á»•i thuá»™c tÃ­nh node thÃ nh text Ä‘á»ƒ encode"""
        text_parts = []
        
        # Láº¥y cÃ¡c thuá»™c tÃ­nh quan trá»ng
        if 'name' in node_attrs:
            text_parts.append(f"name: {node_attrs['name']}")
        if 'companyName' in node_attrs:
            text_parts.append(f"company: {node_attrs['companyName']}")
        if 'contactName' in node_attrs:
            text_parts.append(f"contact: {node_attrs['contactName']}")
        if 'description' in node_attrs:
            text_parts.append(f"description: {node_attrs['description']}")
        if 'city' in node_attrs:
            text_parts.append(f"city: {node_attrs['city']}")
        if 'country' in node_attrs:
            text_parts.append(f"country: {node_attrs['country']}")
        if 'type' in node_attrs:
            text_parts.append(f"type: {node_attrs['type']}")
        
        # Náº¿u khÃ´ng cÃ³ text, dÃ¹ng type
        if not text_parts and 'type' in node_attrs:
            text_parts.append(node_attrs['type'])
        
        return " | ".join(text_parts) if text_parts else "unknown node"
    
    def _create_text_embeddings(self) -> Dict[str, np.ndarray]:
        """Táº¡o text embeddings báº±ng sentence-transformers"""
        print("ğŸ”„ Táº¡o text embeddings cho nodes...")
        
        node_texts = []
        node_ids = []
        
        for node_id, attrs in self.graph_builder.node_attributes.items():
            text = self._encode_node_text(attrs)
            node_texts.append(text)
            node_ids.append(node_id)
            print(f"  ğŸ“ {node_id}: {text}")
        
        # Encode táº¥t cáº£ texts
        print("ğŸ”„ Encoding vá»›i sentence transformer...")
        embeddings_array = self.sentence_model.encode(node_texts, show_progress_bar=True)
        
        # Chuyá»ƒn vá» dict
        embeddings_dict = {}
        for i, node_id in enumerate(node_ids):
            embeddings_dict[node_id] = embeddings_array[i]
        
        print(f"âœ… ÄÃ£ táº¡o text embeddings cho {len(embeddings_dict)} nodes")
        return embeddings_dict
    
    def _prepare_pyg_data(self) -> Data:
        """Chuáº©n bá»‹ dá»¯ liá»‡u cho PyTorch Geometric"""
        print("ğŸ”„ Chuáº©n bá»‹ dá»¯ liá»‡u PyTorch Geometric...")
        
        # Táº¡o mapping tá»« node_id sang index
        node_to_idx = {node: i for i, node in enumerate(self.graph.nodes())}
        self.node_to_idx = node_to_idx
        
        # Node features (text embeddings)
        node_features = []
        for node in self.graph.nodes():
            node_features.append(self.text_embeddings[node])
        
        x = torch.FloatTensor(np.array(node_features))
        
        # Edge index
        edge_index = []
        for edge in self.graph.edges():
            src_idx = node_to_idx[edge[0]]
            dst_idx = node_to_idx[edge[1]]
            edge_index.append([src_idx, dst_idx])
            edge_index.append([dst_idx, src_idx])  # Undirected graph
        
        edge_index = torch.LongTensor(edge_index).t().contiguous()
        
        data = Data(x=x, edge_index=edge_index)
        print(f"âœ… PyG Data: {data.num_nodes} nodes, {data.num_edges} edges")
        
        return data
    
    def _train_gcn(self) -> AdvancedGCN:
        """Huáº¥n luyá»‡n GCN model"""
        print("ğŸ”„ Khá»Ÿi táº¡o vÃ  huáº¥n luyá»‡n GCN...")
        
        input_dim = self.sentence_model.get_sentence_embedding_dimension()
        model = AdvancedGCN(input_dim=input_dim, hidden_dim=128, output_dim=64)
        
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        
        model.train()
        for epoch in range(50):
            optimizer.zero_grad()
            
            # Forward pass
            embeddings = model(self.pyg_data.x, self.pyg_data.edge_index)
            
            # Self-supervised loss: reconstruct node similarities
            reconstructed = torch.mm(embeddings, embeddings.t())
            original_sim = torch.mm(self.pyg_data.x, self.pyg_data.x.t())
            
            loss = F.mse_loss(reconstructed, original_sim)
            loss.backward()
            optimizer.step()
            
            if epoch % 10 == 0:
                print(f"  Epoch {epoch}: Loss = {loss.item():.4f}")
        
        print("âœ… GCN training hoÃ n thÃ nh!")
        model.eval()
        return model
    
    def _create_enhanced_embeddings(self) -> Dict[str, np.ndarray]:
        """Táº¡o enhanced embeddings tá»« trained GCN"""
        print("ğŸ”„ Táº¡o enhanced embeddings tá»« GCN...")
        
        with torch.no_grad():
            enhanced_emb = self.gcn_model(self.pyg_data.x, self.pyg_data.edge_index)
        
        # Chuyá»ƒn vá» dict
        enhanced_embeddings = {}
        for i, node in enumerate(self.graph.nodes()):
            enhanced_embeddings[node] = enhanced_emb[i].numpy()
        
        print("âœ… Enhanced embeddings Ä‘Ã£ sáºµn sÃ ng!")
        return enhanced_embeddings
    
    def find_paths(self, source_nodes: List[str], max_hops: int = 4) -> List[List[str]]:
        """TÃ¬m Ä‘Æ°á»ng Ä‘i báº±ng embedding similarity vÃ  graph structure"""
        if len(source_nodes) < 2:
            return [source_nodes] if source_nodes else []
        
        print("ğŸ”„ TÃ¬m Ä‘Æ°á»ng Ä‘i báº±ng embedding similarity...")
        all_paths = []
        
        # Method 1: TÃ¬m Ä‘Æ°á»ng Ä‘i trá»±c tiáº¿p nhÆ° trÆ°á»›c
        for i in range(len(source_nodes)):
            for j in range(i + 1, len(source_nodes)):
                try:
                    path = nx.shortest_path(self.graph, source_nodes[i], source_nodes[j])
                    if len(path) <= max_hops + 1:
                        all_paths.append(path)
                except nx.NetworkXNoPath:
                    continue
        
        # Method 2: TÃ¬m nodes trung gian dá»±a trÃªn embedding similarity (náº¿u cÃ³ advanced features)
        if ADVANCED_AVAILABLE and hasattr(self, 'enhanced_embeddings'):
            intermediate_nodes = self._find_relevant_intermediate_nodes(source_nodes)
            
            for intermediate in intermediate_nodes[:3]:  # Top 3 most relevant
                for source in source_nodes:
                    try:
                        if intermediate != source:
                            path = nx.shortest_path(self.graph, source, intermediate)
                            if len(path) <= max_hops and path not in all_paths:
                                all_paths.append(path)
                    except nx.NetworkXNoPath:
                        continue
        
        # Method 3: Fallback logic cho trÆ°á»ng há»£p khÃ´ng cÃ³ advanced features
        else:
            node_types = set()
            for node in source_nodes:
                node_type = self.graph.nodes[node].get('type', '')
                node_types.add(node_type)
            
            # Náº¿u cÃ³ Customer vÃ  Product/Category, tÃ¬m Ä‘Æ°á»ng Ä‘i thÃ´ng qua Order
            if 'Customer' in node_types and ('Product' in node_types or 'Category' in node_types):
                customers = [n for n in source_nodes if self.graph.nodes[n].get('type') == 'Customer']
                products = [n for n in source_nodes if self.graph.nodes[n].get('type') == 'Product']
                categories = [n for n in source_nodes if self.graph.nodes[n].get('type') == 'Category']
                
                for customer in customers:
                    for product in products + categories:
                        try:
                            path = nx.shortest_path(self.graph, customer, product)
                            if len(path) <= max_hops + 1:
                                all_paths.append(path)
                        except nx.NetworkXNoPath:
                            continue
        
        return all_paths
    
    def _find_relevant_intermediate_nodes(self, source_nodes: List[str], top_k: int = 5) -> List[str]:
        """TÃ¬m nodes trung gian dá»±a trÃªn embedding similarity"""
        if not source_nodes:
            return []
        
        # TÃ­nh embedding trung bÃ¬nh cá»§a source nodes
        source_embeddings = [self.enhanced_embeddings[node] for node in source_nodes]
        avg_source_embedding = np.mean(source_embeddings, axis=0).reshape(1, -1)
        
        # TÃ­nh similarity vá»›i táº¥t cáº£ nodes
        similarities = {}
        for node_id, embedding in self.enhanced_embeddings.items():
            if node_id not in source_nodes:
                sim = cosine_similarity(avg_source_embedding, embedding.reshape(1, -1))[0][0]
                similarities[node_id] = sim
        
        # Tráº£ vá» top-k similar nodes
        sorted_nodes = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
        print(f"  ğŸ¯ Top relevant intermediate nodes:")
        for node, sim in sorted_nodes[:top_k]:
            attrs = self.graph_builder.node_attributes[node]
            display_name = attrs.get('name') or attrs.get('companyName') or node
            print(f"    - {node} ({display_name}): similarity = {sim:.3f}")
        
        return [node for node, sim in sorted_nodes[:top_k]]
    
    def score_paths(self, paths: List[List[str]]) -> List[Tuple[List[str], float]]:
        """ÄÃ¡nh giÃ¡ Ä‘á»™ quan trá»ng cá»§a cÃ¡c Ä‘Æ°á»ng Ä‘i báº±ng embedding similarity"""
        scored_paths = []
        
        for path in paths:
            if len(path) < 2:
                scored_paths.append((path, 0.0))
                continue
            
            # Base score (ngÆ°á»£c vá»›i Ä‘á»™ dÃ i)
            base_score = 1.0 / len(path)
            
            # Embedding coherence score (náº¿u cÃ³ advanced features)
            if ADVANCED_AVAILABLE and hasattr(self, 'enhanced_embeddings'):
                coherence_score = self._calculate_path_coherence(path)
            else:
                coherence_score = 0.0
            
            # Diversity bonus
            node_types = set()
            for node in path:
                node_type = self.graph.nodes[node].get('type', '')
                node_types.add(node_type)
            diversity_bonus = len(node_types) * 0.1
            
            final_score = base_score + coherence_score + diversity_bonus
            scored_paths.append((path, final_score))
        
        return sorted(scored_paths, key=lambda x: x[1], reverse=True)
    
    def _calculate_path_coherence(self, path: List[str]) -> float:
        """TÃ­nh coherence cá»§a path dá»±a trÃªn embedding similarity"""
        if len(path) < 2:
            return 0.0
        
        similarities = []
        for i in range(len(path) - 1):
            emb1 = self.enhanced_embeddings[path[i]].reshape(1, -1)
            emb2 = self.enhanced_embeddings[path[i + 1]].reshape(1, -1)
            sim = cosine_similarity(emb1, emb2)[0][0]
            similarities.append(sim)
        
        return np.mean(similarities) * 0.5  # Weight factor

class ContextGenerator:
    """BÆ°á»›c 4: Sinh context tá»« Ä‘Æ°á»ng Ä‘i vá»›i enhanced embeddings"""
    
    def __init__(self, graph: nx.Graph, embeddings: Dict[str, np.ndarray] = None):
        self.graph = graph
        self.embeddings = embeddings
    
    def generate_context(self, paths: List[List[str]], question: str = "") -> str:
        """Táº¡o enhanced context tá»« cÃ¡c Ä‘Æ°á»ng Ä‘i vÃ  embeddings"""
        if not paths:
            return "KhÃ´ng tÃ¬m tháº¥y Ä‘Æ°á»ng Ä‘i liÃªn quan trong knowledge graph."
        
        context_parts = []
        if ADVANCED_AVAILABLE and self.embeddings:
            context_parts.append("=== ENHANCED CONTEXT Tá»ª GNN-RAG ===\n")
            
            # Náº¿u cÃ³ cÃ¢u há»i, tÃ¬m nodes most relevant
            if question:
                relevant_info = self._find_question_relevant_info(question, paths)
                context_parts.append("ğŸ¯ ThÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i:")
                context_parts.append(relevant_info)
                context_parts.append("")
        else:
            context_parts.append("=== THÃ”NG TIN Tá»ª KNOWLEDGE GRAPH ===\n")
        
        context_parts.append("ğŸ“ ÄÆ°á»ng Ä‘i Ä‘Æ°á»£c tÃ¬m tháº¥y:")
        
        for i, path in enumerate(paths[:3]):  # Chá»‰ láº¥y 3 Ä‘Æ°á»ng Ä‘i tá»‘t nháº¥t
            coherence_info = ""
            if ADVANCED_AVAILABLE and self.embeddings:
                coherence = self._calculate_path_coherence(path)
                coherence_info = f" (coherence: {coherence:.3f})"
            
            context_parts.append(f"\nÄÆ°á»ng Ä‘i {i+1}{coherence_info}:")
            
            path_description = []
            for j, node in enumerate(path):
                node_attrs = self.graph.nodes[node]
                node_type = node_attrs.get('type', 'Unknown')
                node_name = node_attrs.get('name') or node_attrs.get('companyName') or node
                
                path_description.append(f"{node_type}: {node_name}")
                
                if j < len(path) - 1:
                    # ThÃªm thÃ´ng tin vá» relationship
                    if self.graph.has_edge(path[j], path[j+1]):
                        edge_data = self.graph.edges[path[j], path[j+1]]
                        relation = edge_data.get('relation', 'CONNECTED_TO')
                        path_description.append(f" --[{relation}]--> ")
            
            context_parts.append("  " + " ".join(path_description))
        
        return "\n".join(context_parts)
    
    def _find_question_relevant_info(self, question: str, paths: List[List[str]]) -> str:
        """TÃ¬m thÃ´ng tin relevant vá»›i cÃ¢u há»i"""
        question_lower = question.lower()
        relevant_info = []
        
        # PhÃ¢n tÃ­ch cÃ¡c nodes trong paths
        all_nodes = set()
        for path in paths:
            all_nodes.update(path)
        
        for node in all_nodes:
            attrs = self.graph.nodes[node]
            node_type = attrs.get('type', '')
            
            if 'customer' in question_lower and node_type == 'Customer':
                name = attrs.get('companyName') or attrs.get('contactName')
                relevant_info.append(f"  â€¢ Customer: {name}")
            elif 'product' in question_lower and node_type == 'Product':
                name = attrs.get('name')
                relevant_info.append(f"  â€¢ Product: {name}")
            elif 'category' in question_lower and node_type == 'Category':
                name = attrs.get('name')
                relevant_info.append(f"  â€¢ Category: {name}")
        
        return "\n".join(relevant_info) if relevant_info else "  Tá»± Ä‘á»™ng phÃ¢n tÃ­ch tá»« embeddings"
    
    def _calculate_path_coherence(self, path: List[str]) -> float:
        """TÃ­nh coherence score cá»§a path"""
        if len(path) < 2 or not self.embeddings:
            return 0.0
        
        similarities = []
        for i in range(len(path) - 1):
            if path[i] in self.embeddings and path[i+1] in self.embeddings:
                emb1 = self.embeddings[path[i]].reshape(1, -1)
                emb2 = self.embeddings[path[i+1]].reshape(1, -1)
                sim = cosine_similarity(emb1, emb2)[0][0]
                similarities.append(sim)
        
        return np.mean(similarities) if similarities else 0.0

"Lá»›p Text2CypherGenerator sá»­ dá»¥ng cÃ¡c template Cypher Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a sáºµn (cypher_templates)" 
"vÃ  chá»n template phÃ¹ há»£p dá»±a trÃªn tá»« khÃ³a trong cÃ¢u há»i vÃ  context."
class Text2CypherGenerator:
    """BÆ°á»›c 5: Sinh Cypher query vá»›i context"""
    
    def __init__(self):
        self.cypher_templates = {
            'find_products': """
MATCH (p:Product)
WHERE p.productName CONTAINS '{product_name}'
RETURN p.productName, p.unitPrice
""",
            'find_category_products': """
MATCH (p:Product)-[:PART_OF]->(c:Category)
WHERE c.categoryName = '{category_name}'
RETURN p.productName, p.unitPrice, c.categoryName
""",
            'find_customer_orders': """
MATCH (cust:Customer)-[:PURCHASED]->(o:Order)-[:ORDERS]->(p:Product)
WHERE cust.companyName CONTAINS '{customer_name}' OR cust.contactName CONTAINS '{customer_name}'
RETURN cust.contactName as CustomerName,
       cust.companyName as Company, 
       o.orderDate as OrderDate, 
       p.productName as ProductName
ORDER BY o.orderDate
"""
        }
    
    def generate_cypher(self, question: str, context: str) -> str:
        """Sinh Cypher query dá»±a trÃªn cÃ¢u há»i vÃ  context"""
        question_lower = question.lower()
        
        # PhÃ¢n tÃ­ch xem cÃ³ yÃªu cáº§u liá»‡t kÃª táº¥t cáº£ khÃ´ng
        list_all_keywords = ['list all', 'táº¥t cáº£', 'all', 'list', 'liá»‡t kÃª', 'hiá»ƒn thá»‹']
        is_list_all = any(keyword in question_lower for keyword in list_all_keywords)
        
        if 'sáº£n pháº©m' in question_lower or 'product' in question_lower:
            if is_list_all:
                return """
MATCH (p:Product)
RETURN p.productName, p.unitPrice, p.categoryID
ORDER BY p.productName
"""
            elif 'danh má»¥c' in question_lower or 'category' in question_lower:
                # TÃ¬m trong context Ä‘á»ƒ láº¥y category name
                category_name = 'Beverages'  # default
                if 'beverages' in context.lower() or 'Ä‘á»“ uá»‘ng' in context.lower():
                    category_name = 'Beverages'
                elif 'condiments' in context.lower():
                    category_name = 'Condiments'
                
                return self.cypher_templates['find_category_products'].format(
                    category_name=category_name
                )
            else:
                # TÃ¬m tÃªn sáº£n pháº©m trong context hoáº·c cÃ¢u há»i
                product_name = 'Chai'  # default
                if 'chai' in question_lower:
                    product_name = 'Chai'
                elif 'chang' in question_lower:
                    product_name = 'Chang'
                elif 'syrup' in question_lower:
                    product_name = 'Syrup'
                
                return self.cypher_templates['find_products'].format(
                    product_name=product_name
                )
        
        elif 'khÃ¡ch hÃ ng' in question_lower or 'customer' in question_lower:
            if is_list_all:
                return """
MATCH (c:Customer)
RETURN c.contactName as CustomerName,
       c.companyName as Company, 
       c.city as City, 
       c.country as Country,
       c.phone as Phone
ORDER BY c.contactName
"""
            # Kiá»ƒm tra xem cÃ³ há»i vá» Ä‘á»“ uá»‘ng/beverages khÃ´ng
            elif any(drink_word in question_lower for drink_word in ['Ä‘á»“ uá»‘ng', 'beverage', 'drink']):
                return """
MATCH (cust:Customer)-[:PURCHASED]->(o:Order)-[:ORDERS]->(p:Product)-[:PART_OF]->(c:Category)
WHERE c.categoryName = 'Beverages'
RETURN DISTINCT cust.contactName as CustomerName, 
       cust.companyName as Company,
       cust.city as City
ORDER BY cust.contactName
"""
            else:
                customer_name = 'Alfreds'  # default
                if 'ana' in question_lower:
                    customer_name = 'Ana'
                elif 'alfreds' in question_lower:
                    customer_name = 'Alfreds'
                
                return self.cypher_templates['find_customer_orders'].format(
                    customer_name=customer_name
                )
        
        elif 'danh má»¥c' in question_lower or 'category' in question_lower:
            if is_list_all:
                return """
MATCH (c:Category)
RETURN c.categoryName, c.description
ORDER BY c.categoryName
"""
        
        elif 'Ä‘Æ¡n hÃ ng' in question_lower or 'order' in question_lower:
            if is_list_all:
                return """
MATCH (o:Order)
RETURN o.orderID, o.orderDate, o.customerID
ORDER BY o.orderDate DESC
"""
        
        # Default query cho list all
        if is_list_all:
            return """
MATCH (n)
RETURN labels(n) as NodeType, count(n) as Count
"""
        
        # Default query
        return """
MATCH (n)
RETURN n
LIMIT 10
"""

class GNNRAGDemo:
    """Demo chÃ­nh cá»§a Text2Cypher GNN-RAG"""
    
    def __init__(self):
        print("ğŸš€ Khá»Ÿi táº¡o Advanced GNN-RAG Demo...")
        self.entity_extractor = EntityExtractor()
        self.graph_builder = GraphBuilder()
        
        # Enhanced path finder vá»›i embeddings
        self.path_finder = GNNPathFinder(self.graph_builder.graph, self.graph_builder)
        
        # Enhanced context generator vá»›i embeddings
        embeddings = None
        if hasattr(self.path_finder, 'enhanced_embeddings'):
            embeddings = self.path_finder.enhanced_embeddings
        self.context_generator = ContextGenerator(self.graph_builder.graph, embeddings)
        
        self.cypher_generator = Text2CypherGenerator()
        print("âœ… Advanced GNN-RAG Demo sáºµn sÃ ng!")
    
    def process_question(self, question: str) -> Dict:
        """Xá»­ lÃ½ cÃ¢u há»i theo luá»“ng GNN-RAG"""
        print(f"\nğŸ” CÃ¢u há»i: {question}")
        print("=" * 50)
        
        # BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng
        print("BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Ä‘á»‘i tÆ°á»£ng trong cÃ¢u há»i...")
        entities = self.entity_extractor.extract_entities(question)
        print(f"TÃ¬m tháº¥y {len(entities)} Ä‘á»‘i tÆ°á»£ng:")
        for entity in entities:
            print(f"  - {entity.name} ({entity.entity_type}) - Ä‘á»™ tin cáº­y: {entity.confidence:.2f}")
        
        # BÆ°á»›c 2: TÃ¬m nodes tÆ°Æ¡ng á»©ng
        print("\nBÆ°á»›c 2: TÃ¬m nodes trong knowledge graph...")
        matched_nodes = self.graph_builder.find_entity_nodes(entities)
        print(f"TÃ¬m tháº¥y {len(matched_nodes)} nodes:")
        for node in matched_nodes:
            attrs = self.graph_builder.node_attributes[node]
            display_name = attrs.get('name') or attrs.get('companyName') or 'N/A'
            print(f"  - {node}: {display_name} ({attrs.get('type', 'N/A')})")
        
        # BÆ°á»›c 3: TÃ¬m Ä‘Æ°á»ng Ä‘i báº±ng Advanced GNN
        print("\nBÆ°á»›c 3: TÃ¬m Ä‘Æ°á»ng Ä‘i báº±ng Advanced GNN...")
        paths = self.path_finder.find_paths(matched_nodes)
        scored_paths = self.path_finder.score_paths(paths)
        print(f"TÃ¬m tháº¥y {len(scored_paths)} Ä‘Æ°á»ng Ä‘i:")
        for path, score in scored_paths[:3]:
            print(f"  - Äiá»ƒm: {score:.3f} | ÄÆ°á»ng Ä‘i: {' -> '.join(path)}")
        
        # BÆ°á»›c 4: Táº¡o enhanced context
        print("\nBÆ°á»›c 4: Táº¡o enhanced context tá»« Ä‘Æ°á»ng Ä‘i...")
        best_paths = [path for path, score in scored_paths[:3]]
        context = self.context_generator.generate_context(best_paths, question)
        
        # Náº¿u khÃ´ng cÃ³ Ä‘Æ°á»ng Ä‘i, táº¡o context tá»« matched nodes
        if not best_paths and matched_nodes:
            context_parts = ["=== THÃ”NG TIN Tá»ª KNOWLEDGE GRAPH ===\n"]
            context_parts.append("CÃ¡c nodes liÃªn quan:")
            for node in matched_nodes[:5]:  # Chá»‰ láº¥y 5 nodes Ä‘áº§u
                attrs = self.graph_builder.node_attributes[node]
                node_type = attrs.get('type', 'Unknown')
                node_name = attrs.get('name') or attrs.get('companyName') or node
                context_parts.append(f"- {node_type}: {node_name}")
            context = "\n".join(context_parts)
        
        print("Context Ä‘Ã£ táº¡o:")
        print(context)
        
        # BÆ°á»›c 5: Sinh Cypher query
        print("\nBÆ°á»›c 5: Sinh Cypher query...")
        cypher_query = self.cypher_generator.generate_cypher(question, context)
        print("Cypher query:")
        print(cypher_query)
        
        # ThÃ´ng tin vá» embeddings
        embeddings_info = {}
        if ADVANCED_AVAILABLE and hasattr(self.path_finder, 'enhanced_embeddings'):
            embeddings_info = {
                'text_embedding_dim': self.path_finder.sentence_model.get_sentence_embedding_dimension(),
                'enhanced_embedding_dim': list(self.path_finder.enhanced_embeddings.values())[0].shape[0],
                'total_nodes': len(self.path_finder.enhanced_embeddings),
                'gcn_trained': True
            }
        else:
            embeddings_info = {
                'text_embedding_dim': 0,
                'enhanced_embedding_dim': 64,  # fallback
                'total_nodes': len(matched_nodes),
                'gcn_trained': False
            }
        
        print(f"\nğŸ“Š Embedding Info:")
        print(f"  Text embedding dim: {embeddings_info['text_embedding_dim']}")
        print(f"  Enhanced embedding dim: {embeddings_info['enhanced_embedding_dim']}")
        print(f"  GCN trained: {embeddings_info['gcn_trained']}")
        
        return {
            'question': question,
            'entities': entities,
            'matched_nodes': matched_nodes,
            'paths': scored_paths,
            'context': context,
            'cypher_query': cypher_query,
            'embeddings_info': embeddings_info
        }

def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y demo"""
    print("ğŸš€ DEMO TEXT2CYPHER GNN-RAG")
    print("=" * 50)
    print("Demo nÃ y minh há»a luá»“ng xá»­ lÃ½ tá»« cÃ¢u há»i tiáº¿ng Viá»‡t/Anh Ä‘áº¿n Cypher query")
    print("Sá»­ dá»¥ng knowledge graph vá» há»‡ thá»‘ng quáº£n lÃ½ Ä‘Æ¡n hÃ ng\n")
    
    # Khá»Ÿi táº¡o demo
    demo = GNNRAGDemo()
    
    # CÃ¡c cÃ¢u há»i máº«u
    sample_questions = [
        "TÃ¬m thÃ´ng tin vá» sáº£n pháº©m Chai",
        "KhÃ¡ch hÃ ng nÃ o Ä‘Ã£ mua Ä‘á»“ uá»‘ng?",
        "Sáº£n pháº©m nÃ o thuá»™c danh má»¥c Beverages?",
        "ÄÆ¡n hÃ ng cá»§a khÃ¡ch hÃ ng Alfreds"
    ]
    
    while True:
        print("\n" + "="*50)
        print("MENU:")
        print("1. Chá»n cÃ¢u há»i máº«u")
        print("2. Nháº­p cÃ¢u há»i tÃ¹y chá»‰nh")
        print("3. Xem cáº¥u trÃºc knowledge graph")
        print("4. ThoÃ¡t")
        
        choice = input("\nChá»n lá»±a (1-4): ").strip()
        
        if choice == '1':
            print("\nCÃ¡c cÃ¢u há»i máº«u:")
            for i, q in enumerate(sample_questions, 1):
                print(f"{i}. {q}")
            
            try:
                q_choice = int(input("\nChá»n cÃ¢u há»i (1-4): ")) - 1
                if 0 <= q_choice < len(sample_questions):
                    demo.process_question(sample_questions[q_choice])
                else:
                    print("Lá»±a chá»n khÃ´ng há»£p lá»‡!")
            except ValueError:
                print("Vui lÃ²ng nháº­p sá»‘!")
        
        elif choice == '2':
            question = input("\nNháº­p cÃ¢u há»i cá»§a báº¡n: ").strip()
            if question:
                demo.process_question(question)
            else:
                print("CÃ¢u há»i khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng!")
        
        elif choice == '3':
            print("\nğŸ“Š Cáº¥u trÃºc Knowledge Graph:")
            graph = demo.graph_builder.graph
            print(f"Sá»‘ nodes: {graph.number_of_nodes()}")
            print(f"Sá»‘ edges: {graph.number_of_edges()}")
            
            print("\nCÃ¡c loáº¡i nodes:")
            node_types = {}
            for node in graph.nodes():
                node_type = graph.nodes[node].get('type', 'Unknown')
                node_types[node_type] = node_types.get(node_type, 0) + 1
            
            for node_type, count in node_types.items():
                print(f"  - {node_type}: {count} nodes")
            
            print("\nCÃ¡c loáº¡i relationships:")
            rel_types = {}
            for edge in graph.edges():
                rel_type = graph.edges[edge].get('relation', 'Unknown')
                rel_types[rel_type] = rel_types.get(rel_type, 0) + 1
            
            for rel_type, count in rel_types.items():
                print(f"  - {rel_type}: {count} relationships")
        
        elif choice == '4':
            print("Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng demo! ğŸ‘‹")
            break
        
        else:
            print("Lá»±a chá»n khÃ´ng há»£p lá»‡!")

if __name__ == "__main__":
    main() 